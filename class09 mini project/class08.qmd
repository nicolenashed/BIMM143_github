---
title: "class08 mini project"
author: "Nashed A16631132"
format: pdf
---
```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
head(wisc.df)
```



```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
wisc.data <- wisc.df[,-1]
```

>Q1. How many observations are in this dataset?
There are 569 observations(rows) in this dataset. 

```{r}
nrow(wisc.df)
```
>Q2. How many of the observations have a malignant diagnosis?
There are 212 Malignant diagnosis. 

```{r}
table(wisc.df$diagnosis)
```
```{r}
sum(wisc.df$diagnosis == "M")
```
>Q3. How many variables/features in the data are suffixed with _mean?
There are 10 variables in the data suffixed with _mean. 

```{r}
colnames(wisc.df)
```
```{r}
grep("mean", colnames(wisc.df), value=TRUE)
```
```{r}
x <- colnames(wisc.df)
length( grep("mean", x))
```
#Principal Component Analysis

We need to scale our input data before PCA as some of th ecolumns are measured in terms of very different units with different means and different variances. The upshot here is we set `scale=TRUE` argument to `prcomp()`. 

```{r}
wisc.pr <- prcomp( wisc.data, scale=T )
summary(wisc.pr)
```
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)
```


> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
PC1 account for 44.27% of total variance or .4427. 
> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
PC1,PC2,PC3
> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
PC1,PC2, PC3, PC4, PC5, PC6, PC7

```{r}
biplot(wisc.pr)
```
>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
This plot is very difficult to read and the variable names are overlapping and the color are also very overlapping it does not display meaningful information. 

>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots? This plot shows how PC1 captures a better variance between M and B tumors and the first plot is cleaner and shows the separate clusters more. 

```{r}
plot(wisc.pr$x[, 1], wisc.pr$x[, 3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```
```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```
```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
```{r}
# Variance explained by each principal component: pve
pve <- pr.var/ sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```
>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation[, 1]["concave.points_mean"]
```

>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
PC1, PC2, PC3, PC4, PC5

##3 Hierarchical Clustering

```{r}
data.scaled <- scale(wisc.data)
```

```{r}
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist, method = "complete" )
```
> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```
>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
4 is the better cluster because it has the best number of clusters and gives the best/clear separation of B and M tumors. 

>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
My favorite is ward.D2 because it helps reduce the variance, and the clusters are made even, and groups data points that are alike. 

##5 Combining Methods.

This approach will take not original data but our PCA results and work with them. 

```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method= "ward.D2")
plot(wisc.pr.hclust)
```



Generate 2 cluster groups from this hclust object. 

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
grps
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)
```
```{r}
table(grps)
```
```{r}
table(diagnosis)
```
```{r}
table(diagnosis, grps)
```
```{r}
plot(wisc.pr$x[,1:2], col=grps)
```
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```
```{r}
g <- as.factor(grps)
levels(g)
```
```{r}
g <- relevel(g,2)
levels(g)
```
```{r}
plot(wisc.pr$x[,1:2], col=g)
```
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")
```

```{r}
y <- wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

>Q15. How well does the newly created model with four clusters separate out the two diagnoses?
It separates them out very well and it is organized and clear. 

```{r}
table(y, diagnosis)
```

